@article{SafwanS.Halabi,
author = {{Safwan S. Halabi}, MD • Luciano M. Prevedello, MD • Jayashree Kalpathy-Cramer, PhD • Artem B. Mamonov, PhD • and {Alexander Bilbily}, MD, BHSc • Mark Cicero, MD, BESc, FRCPC • Ian Pan, MA • Lucas Ara{\'{u}}jo Pereira, BSc • and {Rafael Teixeira Sousa}, MSc • Nitamar Abdala, MD, PhD • Felipe Campos Kitamura, MD, MSc • and {Hans H. Thodberg}, PhD • Leon Chen, MD • George Shih, MD • Katherine Andriole, PhD • Marc D. Kohli, MD • and {Bradley J. Erickson}, MD, PhD • Adam E. Flanders, MD},
doi = {10.1148/radiol.2018180736},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Safwan S. Halabi et al. - Unknown - The RSNA Pediatric Bone Age Machine Learning Challenge.pdf:pdf},
title = {{The RSNA Pediatric Bone Age Machine Learning Challenge}},
url = {https://doi.org/10.1148/radiol.2018180736}
}
@article{Smith2018,
abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
archivePrefix = {arXiv},
arxivId = {1803.09820},
author = {Smith, Leslie N.},
eprint = {1803.09820},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Smith - 2018 - A disciplined approach to neural network hyper-parameters Part 1 -- learning rate, batch size, momentum, and weight decay.pdf:pdf},
month = {mar},
title = {{A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay}},
url = {http://arxiv.org/abs/1803.09820},
year = {2018}
}
@techreport{St,
abstract = {Deep learning revolutionized data science, and recently its popularity has grown exponentially, as did the amount of papers employing deep networks. Vision tasks, such as human pose estimation, did not escape from this trend. There is a large number of deep models, where small changes in the network architecture, or in the data pre-processing, together with the stochastic nature of the optimization procedures, produce notably different results, making extremely difficult to sift methods that significantly outperform others. This situation motivates the current study, in which we perform a systematic evaluation and statistical analysis of vanilla deep regression, i.e. convolutional neural networks with a linear regression top layer. This is the first comprehensive analysis of deep regression techniques. We perform experiments on four vision problems, and report confidence intervals for the median performance as well as the statistical significance of the results, if any. Surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modifications in the network architecture. Our results reinforce the hypothesis according to which, in general, a general-purpose network (e.g. VGG-16 or ResNet-50) adequately tuned can yield results close to the state-of-the-art without having to resort to more complex and ad-hoc regression models.},
archivePrefix = {arXiv},
arxivId = {1803.08450v2},
author = {St´, St{\'{e}}phane and Lathuil{\`{i}}, St{\'{e}}phane and Mesejo, Pablo and Alameda-Pineda, Xavier and Horaud, Radu},
eprint = {1803.08450v2},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/St´ et al. - Unknown - A Comprehensive Analysis of Deep Regression.pdf:pdf},
isbn = {1803.08450v2},
keywords = {Computer Vision,Convolutional Neural Networks,Empirical and Systematic Evaluation,Facial Landmark Detection !,Full-Body Pose Estimation,Head-Pose Estimation,Index Terms-Deep Learning,Regression,Statistical Significance},
title = {{A Comprehensive Analysis of Deep Regression}},
url = {https://team.inria.fr/perception/deep-regression/}
}
@techreport{Hea,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385v1},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition(3).pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}}
}
@article{Rana2015,
abstract = {Image Enhancement is one of the most important and complex techniques in image processing technology. The main aim of image enhancement is to improve the visual appearance of an image, or to offer a "better transform representation of the image. Various types of images like medical images, satellite images, aerial images and real life photographs suffer from different problems like poor contrast and noise. It is essential to enhance the contrast and remove the noise to increase image quality. Recently a lot of work has been done by different researchers and scientists in the field of image enhancement. Many techniques have already been proposed and devised for enhancing the digital images. Most of the techniques are based upon the concept of transform domain methods which can introduce the artifacts which further reduce the intensity of the input image. The main objective of this paper is to highlight the drawbacks of the existing techniques.},
author = {Rana, Shashi B},
doi = {10.14741/Ijcet/22774106/5.2.2015.121},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Rana - 2015 - International Journal of Current Engineering and Technology A Review of Medical Image Enhancement Techniques for Image Pro.pdf:pdf},
journal = {1282| International Journal of Current Engineering and Technology},
keywords = {CLAHE,Enhancement,Histograms,Medical Images},
number = {2},
title = {{International Journal of Current Engineering and Technology A Review of Medical Image Enhancement Techniques for Image Processing}},
url = {http://inpressco.com/category/ijcet},
volume = {5},
year = {2015}
}
@techreport{Sta,
abstract = {Deep learning revolutionized data science, and recently its popularity has grown exponentially, as did the amount of papers employing deep networks. Vision tasks, such as human pose estimation, did not escape from this trend. There is a large number of deep models, where small changes in the network architecture, or in the data pre-processing, together with the stochastic nature of the optimization procedures, produce notably different results, making extremely difficult to sift methods that significantly outperform others. This situation motivates the current study, in which we perform a systematic evaluation and statistical analysis of vanilla deep regression, i.e. convolutional neural networks with a linear regression top layer. This is the first comprehensive analysis of deep regression techniques. We perform experiments on four vision problems, and report confidence intervals for the median performance as well as the statistical significance of the results, if any. Surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modifications in the network architecture. Our results reinforce the hypothesis according to which, in general, a general-purpose network (e.g. VGG-16 or ResNet-50) adequately tuned can yield results close to the state-of-the-art without having to resort to more complex and ad-hoc regression models.},
archivePrefix = {arXiv},
arxivId = {1803.08450v2},
author = {St´, St{\'{e}}phane and Lathuil{\`{i}}, St{\'{e}}phane and Mesejo, Pablo and Alameda-Pineda, Xavier and Horaud, Radu},
eprint = {1803.08450v2},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/St´ et al. - Unknown - A Comprehensive Analysis of Deep Regression(2).pdf:pdf},
isbn = {1803.08450v2},
keywords = {Computer Vision,Convolutional Neural Networks,Empirical and Systematic Evaluation,Facial Landmark Detection !,Full-Body Pose Estimation,Head-Pose Estimation,Index Terms-Deep Learning,Regression,Statistical Significance},
title = {{A Comprehensive Analysis of Deep Regression}},
url = {https://team.inria.fr/perception/deep-regression/}
}
@techreport{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@book{RanjayKrishna2017,
abstract = {This book fills this void by including a collection of representative articles, which gives an up-to-date and comprehensive snapshot of the Peer-to-Peer field. One of the main challenges that faces any book covering such a vast and relatively new territory is how to structure the material. This book resolves this conundrum by dividing the material into roughly three parts. The first part of the book covers the basics of Peer-to-Peer designs, un- structured and structured systems, and presents a variety of applications in- cluding e-mail, multicast, Grid computing, andWeb services. The book then goes beyond describing traditional systems, by discussing general aspects of the Peer-to-Peer systems, namely the self-organization nature of the Peer- to-Peer systems, and the all-important topic of evaluating these systems. In addition, the book illustrates the broad applicability of Peer-to-Peer by dis- cussing the impact of the Peer-to-Peer technologies in two computer-science areas, namely searching and information retrieval, and mobile computing. No Peer-to-Peer book would be complete without discussing the business model, accounting, and security. This book touches on these topics in the last part.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Ranjay Krishna}},
booktitle = {CS131},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Ranjay Krishna - 2017 - Computer Vision Stanford(3).pdf:pdf},
issn = {00157120},
keywords = {0415492785 (hardbound),1-4919-0490-9,1491904909,1999,2017,36,6700,9,978-1-4919-0490-9,9780415492782 (hardbound),9781491904909,Action,Adaptivity patterns,Ambient assisting living,Application program interfaces (Computer software),Application software,Arsenic,Artificial intelligence.,Autoregression,B3 perspective,BETWEEN FACTS AND NORMS,Basic characteristics,Bayes smoothing estimation,Big Data,Big data,Big data.,Big-data,Bitcoin,Blockchain,Blockchain business board,Blockchain business innovation,Blockchain technology and management,Business Data processing.,Business model innovation,Client/server computing -- Software,Computational Intelligence.,Computing: The Next 50 Years,Copyright protection,Cryptocurrency,Data mining,Data mining.,Data science,Data-mining visualization,Digital currencies,Digital inequality,Digital innovation,Digital map,Electric wiring,Electronic books,Engineeri,Engineering.,Estimation of correlation,Fall-detection,Geographic information system (GIS),Gynaecology,Habermas,High-dimensional information,Hydride generation atomic absorption spectrometry,ISBN-13: 9780321529176",Information Overload,Information systems,Information technology Management.,Information visualization.,Interior.,Internet of Things,IoT,IoT self-adaptive,Jeff Patton,Kalman filter,Machine learning,Mathematical models,Mayer-v. Rokitansky-K{\"{u}}ster (-Hauser) syndrome,Mental health,Mobile business,O'Reilly,Obstetrics,Persona,Personalized care,Peter  Economy,Precision medicine,Precision psychiatry,Predictive analytics,Psychiatry,Representational State Transfer (Software architec,Robot development,Rokitansky,SLIS,SaaS,Scenario,Sequential injection,Spondylolisthesis,State pace model,TAM,Technology,Tika,Time series analysis,UCSD,User Experience,User Story Mapping,Vector data,Watermarking,Web site development,Web sites / Design,Web-based risk calculators,adaptive technology-enhanced learning,also,antigen,art of woodworking,autobiographical memory,biologicamente ativos que s{\~{a}}o,bola,bola-drb3,bovine,called the bovine leucocyte,case study,cloud,complex,computer aided instruction,computer games,computer supported collaborative learning,cybersecurity,deliberative democracy,dependentes das suas estruturas,depression,design thinking,design thinking minds,digital game,digital systems,e-learning,emergency management system,emotional aspects,executive control,formal education,genotyping,governance and technology series,groupware,handicapped aids,icle,includes many immune-related genes,informal education,innovation management,intelligent toy enhanced learning,international standards,knowledge management,law,learning analytics,lewin et al,magnitude estimation,measurement,mhc,mobile augmented reality,mobile computing,mobile technologies,motivational aspects,multiple criteria decision-making,natural language processing,o'reilly,of cattle,open access,oreilly,overgeneral memory,people with disabilities,personalised technology-enhanced learning,recommender systems,security,semantic Web,semantic Web technologies,singrar,smart learning environments,social networking,software as a service,technology-enhanced Science,technology-enhanced assessment,technology-enhanced language learning,technology-supported educational innovation,the major histocompatibility complex,ubiquitous technologies,urban experience,usability,usability study,user experience,virtual worlds,wireless technologies,working,www.it-ebooks.info},
number = {5},
pages = {43},
pmid = {25246403},
title = {{Computer Vision Stanford}},
volume = {91},
year = {2017}
}
@article{Smith2017,
abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
archivePrefix = {arXiv},
arxivId = {1708.07120},
author = {Smith, Leslie N. and Topin, Nicholay},
eprint = {1708.07120},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Smith, Topin - 2017 - Super-Convergence Very Fast Training of Neural Networks Using Large Learning Rates.pdf:pdf},
month = {aug},
pages = {36},
publisher = {SPIE-Intl Soc Optical Eng},
title = {{Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates}},
url = {http://arxiv.org/abs/1708.07120},
year = {2017}
}
@techreport{Luo2018,
abstract = {Adaptive optimization methods such as ADAGRAD, RMSPROP and ADAM have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGRAD to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of ADAM and AMSGRAD, called ADABOUND and AMSBOUND respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/AdaBound.},
author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu and Key, Moe},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Luo et al. - 2018 - ADAPTIVE GRADIENT METHODS WITH DYNAMIC BOUND OF LEARNING RATE.pdf:pdf},
month = {sep},
title = {{ADAPTIVE GRADIENT METHODS WITH DYNAMIC BOUND OF LEARNING RATE}},
url = {https://github.com/Luolc/AdaBound.},
year = {2018}
}
@techreport{Tan,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https: //github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1905.11946v3},
author = {Tan, Mingxing and Le, Quoc V},
eprint = {1905.11946v3},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Tan, Le - Unknown - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:pdf},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch normalization Accelerating deep network training by reducing internal covariate shift.pdf:pdf},
isbn = {9781510810587},
month = {feb},
pages = {448--456},
publisher = {International Machine Learning Society (IMLS)},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
volume = {1},
year = {2015}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:pdf},
month = {dec},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Smith2015,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
eprint = {1506.01186},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Smith - 2015 - Cyclical Learning Rates for Training Neural Networks(2).pdf:pdf},
journal = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
pages = {464--472},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Cyclical Learning Rates for Training Neural Networks}},
year = {2015}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{Ulyanov2016,
abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture{\_}nets. Full paper can be found at arXiv:1701.02096.},
archivePrefix = {arXiv},
arxivId = {1607.08022},
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
eprint = {1607.08022},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Ulyanov, Vedaldi, Lempitsky - 2016 - Instance Normalization The Missing Ingredient for Fast Stylization.pdf:pdf},
month = {jul},
title = {{Instance Normalization: The Missing Ingredient for Fast Stylization}},
url = {http://arxiv.org/abs/1607.08022},
year = {2016}
}
@techreport{Wu,
abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems-BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6{\%} lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normaliza-tion variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, 1 and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
archivePrefix = {arXiv},
arxivId = {1803.08494v3},
author = {Wu, Yuxin and He, Kaiming},
eprint = {1803.08494v3},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Wu, He - Unknown - Group Normalization(2).pdf:pdf},
title = {{Group Normalization}}
}
@article{Wilson2017,
abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
archivePrefix = {arXiv},
arxivId = {1705.08292},
author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
eprint = {1705.08292},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Wilson et al. - 2017 - The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {may},
pages = {4149--4159},
publisher = {Neural information processing systems foundation},
title = {{The Marginal Value of Adaptive Gradient Methods in Machine Learning}},
url = {http://arxiv.org/abs/1705.08292},
volume = {2017-December},
year = {2017}
}
