Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Hea,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8Ã— deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385v1},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition(3).pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}}
}
@techreport{Wu,
abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems-BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6{\%} lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normaliza-tion variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, 1 and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
archivePrefix = {arXiv},
arxivId = {1803.08494v3},
author = {Wu, Yuxin and He, Kaiming},
eprint = {1803.08494v3},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Wu, He - Unknown - Group Normalization(2).pdf:pdf},
title = {{Group Normalization}}
}
@book{RanjayKrishna2017,
abstract = {This book fills this void by including a collection of representative articles, which gives an up-to-date and comprehensive snapshot of the Peer-to-Peer field. One of the main challenges that faces any book covering such a vast and relatively new territory is how to structure the material. This book resolves this conundrum by dividing the material into roughly three parts. The first part of the book covers the basics of Peer-to-Peer designs, un- structured and structured systems, and presents a variety of applications in- cluding e-mail, multicast, Grid computing, andWeb services. The book then goes beyond describing traditional systems, by discussing general aspects of the Peer-to-Peer systems, namely the self-organization nature of the Peer- to-Peer systems, and the all-important topic of evaluating these systems. In addition, the book illustrates the broad applicability of Peer-to-Peer by dis- cussing the impact of the Peer-to-Peer technologies in two computer-science areas, namely searching and information retrieval, and mobile computing. No Peer-to-Peer book would be complete without discussing the business model, accounting, and security. This book touches on these topics in the last part.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Ranjay Krishna}},
booktitle = {CS131},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Ranjay Krishna - 2017 - Computer Vision Stanford(3).pdf:pdf},
issn = {00157120},
keywords = {0415492785 (hardbound),1-4919-0490-9,1491904909,1999,2017,36,6700,9,978-1-4919-0490-9,9780415492782 (hardbound),9781491904909,Action,Adaptivity patterns,Ambient assisting living,Application program interfaces (Computer software),Application software,Arsenic,Artificial intelligence.,Autoregression,B3 perspective,BETWEEN FACTS AND NORMS,Basic characteristics,Bayes smoothing estimation,Big Data,Big data,Big data.,Big-data,Bitcoin,Blockchain,Blockchain business board,Blockchain business innovation,Blockchain technology and management,Business Data processing.,Business model innovation,Client/server computing -- Software,Computational Intelligence.,Computing: The Next 50 Years,Copyright protection,Cryptocurrency,Data mining,Data mining.,Data science,Data-mining visualization,Digital currencies,Digital inequality,Digital innovation,Digital map,Electric wiring,Electronic books,Engineeri,Engineering.,Estimation of correlation,Fall-detection,Geographic information system (GIS),Gynaecology,Habermas,High-dimensional information,Hydride generation atomic absorption spectrometry,ISBN-13: 9780321529176",Information Overload,Information systems,Information technology Management.,Information visualization.,Interior.,Internet of Things,IoT,IoT self-adaptive,Jeff Patton,Kalman filter,Machine learning,Mathematical models,Mayer-v. Rokitansky-K{\"{u}}ster (-Hauser) syndrome,Mental health,Mobile business,O'Reilly,Obstetrics,Persona,Personalized care,Peter  Economy,Precision medicine,Precision psychiatry,Predictive analytics,Psychiatry,Representational State Transfer (Software architec,Robot development,Rokitansky,SLIS,SaaS,Scenario,Sequential injection,Spondylolisthesis,State pace model,TAM,Technology,Tika,Time series analysis,UCSD,User Experience,User Story Mapping,Vector data,Watermarking,Web site development,Web sites / Design,Web-based risk calculators,adaptive technology-enhanced learning,also,antigen,art of woodworking,autobiographical memory,biologicamente ativos que s{\~{a}}o,bola,bola-drb3,bovine,called the bovine leucocyte,case study,cloud,complex,computer aided instruction,computer games,computer supported collaborative learning,cybersecurity,deliberative democracy,dependentes das suas estruturas,depression,design thinking,design thinking minds,digital game,digital systems,e-learning,emergency management system,emotional aspects,executive control,formal education,genotyping,governance and technology series,groupware,handicapped aids,icle,includes many immune-related genes,informal education,innovation management,intelligent toy enhanced learning,international standards,knowledge management,law,learning analytics,lewin et al,magnitude estimation,measurement,mhc,mobile augmented reality,mobile computing,mobile technologies,motivational aspects,multiple criteria decision-making,natural language processing,o'reilly,of cattle,open access,oreilly,overgeneral memory,people with disabilities,personalised technology-enhanced learning,recommender systems,security,semantic Web,semantic Web technologies,singrar,smart learning environments,social networking,software as a service,technology-enhanced Science,technology-enhanced assessment,technology-enhanced language learning,technology-supported educational innovation,the major histocompatibility complex,ubiquitous technologies,urban experience,usability,usability study,user experience,virtual worlds,wireless technologies,working,www.it-ebooks.info},
number = {5},
pages = {43},
pmid = {25246403},
title = {{Computer Vision Stanford}},
volume = {91},
year = {2017}
}
@article{Smith2015,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
eprint = {1506.01186},
file = {:Users/luischavesrodriguez/Library/Application Support/Mendeley Desktop/Downloaded/Smith - 2015 - Cyclical Learning Rates for Training Neural Networks(2).pdf:pdf},
journal = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
pages = {464--472},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Cyclical Learning Rates for Training Neural Networks}},
year = {2015}
}
